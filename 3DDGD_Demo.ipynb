{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "OTAyuV4oeG_0",
        "YtMV1PU4ekEo",
        "UBIehXHwd6VI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 3D Deepfake Generation and Detection\n",
        "\n",
        "# Authored by Hichem Felouat\n",
        "# Email            : hichemfel@nii.ac.jp\n",
        "# GitHub Repository: https://github.com/hichemfelouat/3DDGD.git\n",
        "\n"
      ],
      "metadata": {
        "id": "O2ceyt1SF3DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyLcfqHL6qnZ",
        "outputId": "2e7c025e-dd30-4859-bdb8-6fde1c51cc77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun 13 04:51:49 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clone Repository:**"
      ],
      "metadata": {
        "id": "OTAyuV4oeG_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hichemfelouat/3DDGD.git"
      ],
      "metadata": {
        "id": "xtI58uEbom5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 3DDGD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4glxJbMqo70T",
        "outputId": "8a7be98f-dfc6-4396-c235-821e3778da95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/3DDGD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/3DDGD/weights.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXyCxuNXqVoY",
        "outputId": "f9958a4d-01de-4cae-9d6f-aa5e8c206466"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/3DDGD/weights.zip\n",
            "  inflating: weights/Tabtransformer/Tabtransformer_G0.pth  \n",
            "  inflating: weights/Tabtransformer/Tabtransformer_G1.pth  \n",
            "  inflating: weights/Tabtransformer/Tabtransformer_G2.pth  \n",
            "  inflating: weights/Mesh_MLP_MHA/Mesh_MLP_MHA_G1.pth  \n",
            "  inflating: weights/Mesh_MLP_MHA/Mesh_MLP_MHA_G2.pth  \n",
            "  inflating: weights/Mesh_MLP_MHA/Mesh_MLP_MHA_G0.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Requirements:**"
      ],
      "metadata": {
        "id": "YtMV1PU4ekEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!pip install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=12.1 -c pytorch -c nvidia\n",
        "\n",
        "!pip install trimesh\n",
        "!pip install open3d\n",
        "!pip install libigl\n",
        "!pip install robust-Laplacian\n",
        "\n",
        "!pip install scipy==1.11.3\n",
        "!pip install shapely==2.0.3\n",
        "!pip install pyglet==1.5.27\n",
        "!pip install mediapipe==0.10.11\n",
        "!pip install timm==1.0.9\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rnz_yAryTlYN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e00f2fc6-85b5-4dbc-f6d3-0a72d6919a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=12.1 -c pytorch -c nvidia\\n\\n!pip install trimesh\\n!pip install open3d\\n!pip install libigl\\n!pip install robust-Laplacian\\n\\n!pip install scipy==1.11.3\\n!pip install shapely==2.0.3\\n!pip install pyglet==1.5.27\\n!pip install mediapipe==0.10.11\\n!pip install timm==1.0.9\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh\n",
        "!pip install open3d\n",
        "!pip install libigl\n",
        "!pip install robust-Laplacian\n",
        "#!pip install scipy\n",
        "\n",
        "# For face cropping\n",
        "!pip install scipy==1.11.3\n",
        "!pip install shapely==2.0.3\n",
        "!pip install pyglet==1.5.27\n",
        "!pip install mediapipe==0.10.11\n",
        "!pip install timm==1.0.9\n"
      ],
      "metadata": {
        "id": "1Qc89uR-gE53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3DDGD Inference:**"
      ],
      "metadata": {
        "id": "UBIehXHwd6VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can download a free .obj-format example file here.\n",
        "# https://www.artec3d.com/3d-models/head\n",
        "# Inference\n",
        "!python inference.py  \\\n",
        "--input_data examples  \\\n",
        "--model_name Mesh_MLP_MHA  \\\n",
        "--feature_type G0  \\\n",
        "--is_cropped 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb1MeTuB_DPm",
        "outputId": "0f112800-5d88-4a09-eb93-af6068b682fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-13 05:50:33.353347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749793833.373286   18410 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749793833.379486   18410 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-13 05:50:33.400660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[H\u001b[2J\n",
            "Start ...\n",
            "Input Data  :  examples\n",
            "Feature Type:  G0\n",
            "Model Name  :  Mesh_MLP_MHA\n",
            "Is Cropped  :  0\n",
            "k      :  16\n",
            "in_dim :  624\n",
            "Load the model ... ( Mesh_MLP_MHA )\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "  0% 0/1 [00:00<?, ?it/s]0  - man_bust.obj : \n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "man_bust.obj cropped is OK.\n",
            "Vertices: (7780, 3), Faces: (14999, 3)\n",
            "Data Preparation Time (s)            :  91.04655432701111\n",
            "Extrinsic Features ...\n",
            "Extrinsic Feature Extraction Time (s):  0.3086361885070801\n",
            "Intrinsic Features ...\n",
            "features :  torch.Size([7780, 39])\n",
            "Input features :  624\n",
            "Intrinsic Feature Extraction Time (s):  8.367841243743896\n",
            "Inference ...\n",
            "Model output : man_bust.obj is Real. ( 0.024476613849401474 )\n",
            "Inference Time (s)                   :  0.034877777099609375\n",
            "--------------------------------------------------\n",
            "100% 1/1 [01:39<00:00, 99.76s/it]\n",
            "Total Execution Time (s):  100.41026306152344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3DDGD Inference With Gradio:**"
      ],
      "metadata": {
        "id": "_0a84-Whe3jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Functions:**"
      ],
      "metadata": {
        "id": "IZb445wZ7J-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/3DDGD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTTZ9ary-JyO",
        "outputId": "a244cf4c-9726-4cd8-ff40-6c6670a95d9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/3DDGD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import trimesh\n",
        "from trimesh import Trimesh\n",
        "from glob import glob\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import trimesh\n",
        "from trimesh import Trimesh\n",
        "\n",
        "from crop_3Dface.crop_3D_face import*\n",
        "from crop_3Dface.compute_3d_landmarks import compute_landmarks_pc\n",
        "from crop_3Dface.obj_handler import read_obj_file\n",
        "\n",
        "from features_3Dmesh.features_3d_mesh import*\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pickle import load\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from Tabtransformer.tabtransformer import *\n",
        "from Mesh_MLP_MHA.meshmlp_mha_net import *\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "def crop3Dface(obj_file, is_from_faceScape=0):\n",
        "\n",
        "  mesh = np.asarray(read_obj_file(obj_file))\n",
        "\n",
        "  # 3D facial landmarks extraction\n",
        "  lm_nocrop, lm_crop = compute_landmarks_pc(\n",
        "    mesh,\n",
        "    img_size          = 256,\n",
        "    cropping          = True,\n",
        "    img_save_nocrop   = None,\n",
        "    img_save_crop     = None,\n",
        "    visibility_radius = 0,\n",
        "    visibility_obj    = None)\n",
        "\n",
        "  # List of landmarks\n",
        "  try:\n",
        "    if lm_crop == None:\n",
        "      #print(\"len lm_nocrop : \",len(lm_nocrop))\n",
        "      lm_crop = lm_nocrop.copy()\n",
        "  except:\n",
        "    #print(\"len lm_nocrop : \",len(lm_nocrop))\n",
        "    #print(\"len lm_crop   : \",len(lm_crop))\n",
        "    alpha   = 1.0\n",
        "\n",
        "  lst_eye_l_ind  = [29,27,28,56,157,153,145,24,110,25,33]\n",
        "  lst_eye_r_ind  = [286,258,257,388,390,254,253,252,256,463,414]\n",
        "\n",
        "  # Get cropped face\n",
        "  alpha   = 1.0\n",
        "  mrg     = 0.0001\n",
        "  if is_from_faceScape == 0:\n",
        "    mrg = 0.0001\n",
        "  else:\n",
        "    mrg = 3.00\n",
        "\n",
        "  cropped_face = crop_face_from_obj(obj_file, lm_crop, lst_eye_l_ind, lst_eye_r_ind, mrg, alpha, is_from_faceScape)\n",
        "\n",
        "  # Remove irrelevant parts\n",
        "  cropped_face_irr_p = remove_irrelevant_parts(cropped_face)\n",
        "\n",
        "  # Remove eyes outliers\n",
        "  vertices_face   = cropped_face_irr_p.vertices\n",
        "  faces_face      = cropped_face_irr_p.faces\n",
        "  vertices, faces = remove_eyes_outlier(vertices_face.tolist(), faces_face.tolist(),  lm_crop, lst_eye_l_ind, lst_eye_r_ind, 0.001)\n",
        "\n",
        "  vertices_arr = np.array(vertices)\n",
        "  faces_arr    = np.array(faces)\n",
        "  cropped_face_new = Trimesh(vertices_arr, faces_arr)\n",
        "\n",
        "  return cropped_face_new, lm_crop\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "class CustomInferenceDataset(Dataset):\n",
        "  def __init__(self, features, scaler_path=None):\n",
        "\n",
        "    self.features = features\n",
        "\n",
        "    # Load the saved scaler\n",
        "    if scaler_path is not None:\n",
        "        self.scaler = load(open(scaler_path, \"rb\"))\n",
        "        # Apply the scaler to the features\n",
        "        self.features = self.scaler.transform(np.array(features).reshape(1, -1))\n",
        "\n",
        "  def __len__(self):\n",
        "    return 1  # Since it's a single example\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.features, dtype=torch.float32)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def error_message(error_type):\n",
        "  print(\"****************************\")\n",
        "  if error_type == 1:\n",
        "    print(\"Error: The model did not load correctly. Ensure the path is correct and the model is in the specified location.\")\n",
        "  elif error_type == 2:\n",
        "    print(\"Error: An error occurred during the 3D face cropping process.\")\n",
        "  elif error_type == 3:\n",
        "    print(\"Error: An error occurred during the feature extraction process.\")\n",
        "  else:\n",
        "    print(\"Error: An error occurred during the prediction process.\")\n",
        "  print(\"****************************\")\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def load_model(model_name, feature_dim):\n",
        "  # Load the model\n",
        "  print(\"Load the model ... (\",model_name,\")\")\n",
        "\n",
        "  if model_name == \"Tabtransformer\":\n",
        "      try:\n",
        "        if feature_dim == 624:\n",
        "          model_path = \"weights/Tabtransformer/Tabtransformer_G0.pth\"\n",
        "        if feature_dim == 2496:\n",
        "          model_path = \"weights/Tabtransformer/Tabtransformer_G1.pth\"\n",
        "        if feature_dim == 6006:\n",
        "          model_path = \"weights/Tabtransformer/Tabtransformer_G2.pth\"\n",
        "\n",
        "        d_model      = 128\n",
        "        num_heads    = 4\n",
        "        num_layers   = 3\n",
        "        d_ff         = 256\n",
        "        num_classes  = 1\n",
        "        dropout_rate = 0.1\n",
        "\n",
        "        model_tab = TabTransformer(feature_dim, d_model, num_heads, num_layers, d_ff, num_classes, dropout_rate)\n",
        "        model_tab.load_state_dict(torch.load(model_path))\n",
        "        model_tab.eval()\n",
        "\n",
        "        return model_tab\n",
        "\n",
        "      except:\n",
        "        error_type = 1\n",
        "        error_message(error_type)\n",
        "        print(\"model_path : \",model_path)\n",
        "        sys.exit(1)\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  if model_name == \"Mesh_MLP_MHA\":\n",
        "      try:\n",
        "        if feature_dim == 624:\n",
        "          model_path = \"weights/Mesh_MLP_MHA/Mesh_MLP_MHA_G0.pth\"\n",
        "        if feature_dim == 2496:\n",
        "          model_path = \"weights/Mesh_MLP_MHA/Mesh_MLP_MHA_G1.pth\"\n",
        "        if feature_dim == 6006:\n",
        "          model_path = \"weights/Mesh_MLP_MHA/Mesh_MLP_MHA_G2.pth\"\n",
        "\n",
        "        num_classes = 1\n",
        "        drop_prob   = 0.1\n",
        "        k_eig_list  = [2047, 128, 32]\n",
        "\n",
        "        model_MHA = Net(C_in=feature_dim, C_out=num_classes, drop_path_rate=drop_prob, k_eig_list=k_eig_list)\n",
        "        model_MHA.load_state_dict(torch.load(model_path))\n",
        "        model_MHA.eval()\n",
        "\n",
        "        return model_MHA\n",
        "\n",
        "      except:\n",
        "        error_type = 1\n",
        "        error_message(error_type)\n",
        "        print(\"model_path : \",model_path)\n",
        "        sys.exit(1)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "print(\"Done ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "InbQbZPsjFHq",
        "outputId": "21c73cc8-6eee-4efb-bc3b-1f9f6adb6215"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_3d_mesh(mesh):\n",
        "    # Extract vertices and faces from the mesh\n",
        "    vertices = mesh.vertices\n",
        "    faces    = mesh.faces\n",
        "\n",
        "    # Create mesh3d trace for the face\n",
        "    face_trace = go.Mesh3d(\n",
        "        x=vertices[:, 0],\n",
        "        y=vertices[:, 1],\n",
        "        z=vertices[:, 2],\n",
        "        i=faces[:, 0],\n",
        "        j=faces[:, 1],\n",
        "        k=faces[:, 2],\n",
        "        colorscale=[[0, 'rgb(150, 150, 150)'], [1, 'rgb(210, 210, 210)']],\n",
        "        intensity=vertices[:, 2],  # Use z-coordinate for shading\n",
        "        intensitymode='vertex',\n",
        "        lighting=dict(\n",
        "            ambient=0.6,\n",
        "            diffuse=0.5,\n",
        "            fresnel=0.1,\n",
        "            specular=0.2,\n",
        "            roughness=0.1\n",
        "        ),\n",
        "        lightposition=dict(x=100, y=200, z=0),\n",
        "        name='Face Mesh'\n",
        "    )\n",
        "\n",
        "    data = [face_trace]\n",
        "\n",
        "    # Define camera for frontal z-axis view\n",
        "    camera = dict(\n",
        "        eye   =dict(x=0, y=0, z=2),  # Camera is positioned along the z-axis\n",
        "        center=dict(x=0, y=0, z=0),  # Looking at the center of the scene\n",
        "        up    =dict(x=0, y=1, z=0))  # 'Up' direction is along the y-axis\n",
        "\n",
        "    # Create the layout with specific width and height\n",
        "    layout = go.Layout(\n",
        "        scene_camera=camera,\n",
        "        scene=dict(\n",
        "            xaxis=dict(visible=False),\n",
        "            yaxis=dict(visible=False),\n",
        "            zaxis=dict(visible=False),\n",
        "            aspectmode='data'\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=0),\n",
        "        width =500,\n",
        "        height=300\n",
        "    )\n",
        "\n",
        "    # Create the figure\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "    return fig\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def plot_3d_mesh_landmarks(mesh_path, landmarks):\n",
        "    # Load the mesh\n",
        "    mesh     = trimesh.load(mesh_path)\n",
        "    vertices = mesh.vertices\n",
        "    faces    = mesh.faces\n",
        "\n",
        "    # Create mesh3d trace for the face\n",
        "    face_trace = go.Mesh3d(\n",
        "        x=vertices[:, 0],\n",
        "        y=vertices[:, 1],\n",
        "        z=vertices[:, 2],\n",
        "        i=faces[:, 0],\n",
        "        j=faces[:, 1],\n",
        "        k=faces[:, 2],\n",
        "        colorscale=[[0, 'rgb(150, 150, 150)'], [1, 'rgb(210, 210, 210)']],\n",
        "        intensity=vertices[:, 2],  # Use z-coordinate for shading\n",
        "        intensitymode='vertex',\n",
        "        lighting=dict(\n",
        "            ambient=0.6,\n",
        "            diffuse=0.5,\n",
        "            fresnel=0.1,\n",
        "            specular=0.2,\n",
        "            roughness=0.1\n",
        "        ),\n",
        "        lightposition=dict(x=100, y=200, z=0),\n",
        "        name='Face Mesh'\n",
        "    )\n",
        "\n",
        "    # Create scatter3d trace for landmarks\n",
        "    landmark_trace = go.Scatter3d(\n",
        "        x=landmarks[:, 0],\n",
        "        y=landmarks[:, 1],\n",
        "        z=landmarks[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size  =3,\n",
        "            color ='red',\n",
        "            symbol='circle'\n",
        "        ),\n",
        "        name='Landmarks'\n",
        "    )\n",
        "\n",
        "    # Combine both traces\n",
        "    data = [face_trace, landmark_trace]\n",
        "\n",
        "    # Define camera for frontal z-axis view\n",
        "    camera = dict(\n",
        "        eye   =dict(x=0, y=0, z=2),  # Camera is positioned along the z-axis\n",
        "        center=dict(x=0, y=0, z=0),  # Looking at the center of the scene\n",
        "        up    =dict(x=0, y=1, z=0))  # 'Up' direction is along the y-axis\n",
        "\n",
        "    # Create the layout with specific width and height\n",
        "    layout = go.Layout(\n",
        "        scene_camera=camera,\n",
        "        scene=dict(\n",
        "            xaxis=dict(visible=False),\n",
        "            yaxis=dict(visible=False),\n",
        "            zaxis=dict(visible=False),\n",
        "            aspectmode='data'\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=0),\n",
        "        width =500,\n",
        "        height=300\n",
        "    )\n",
        "\n",
        "    # Create the figure\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "    return fig\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def crop_3D_face(mesh_path, is_cropped):\n",
        "  face_obj_path = mesh_path\n",
        "  name_face_obj = face_obj_path.split(\"/\")[-1]\n",
        "  print(name_face_obj+\" : \")\n",
        "\n",
        "  try:\n",
        "    if is_cropped == 0:\n",
        "      cropped_face  = crop3Dface(face_obj_path)\n",
        "      print(name_face_obj+\" cropped is OK.\")\n",
        "      #cropped_face.export(file_obj=input_data+\"/cropped_\"+name_face_obj, file_type=\"obj\")\n",
        "      return cropped_face\n",
        "    else:\n",
        "      cropped_face = trimesh.load(face_obj_path)\n",
        "      print(name_face_obj+\" loaded is OK.\")\n",
        "      return cropped_face\n",
        "\n",
        "  except:\n",
        "    error_type = 2\n",
        "    error_message(error_type)\n",
        "    #sys.exit(1)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def model_predict(cropped_face, model_name, feature_type):\n",
        "\n",
        "  device = \"cuda\"\n",
        "  k      = 16\n",
        "  in_dim = 624\n",
        "\n",
        "  if feature_type == \"G1\":\n",
        "    k = 64\n",
        "    in_dim = 2496\n",
        "\n",
        "  if feature_type == \"G2\":\n",
        "    k = 154\n",
        "    in_dim = 6006\n",
        "\n",
        "  error_type = 0\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_loaded = load_model(model_name, in_dim)\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  if error_type == 0:\n",
        "    try:\n",
        "      face_mesh = normalize_faces_from_mesh(cropped_face, 15000)\n",
        "      face_mesh = normalize_mesh(face_mesh)\n",
        "\n",
        "      print(\"Extrinsic Features ...\")\n",
        "      vertex_coordinates = get_vertex_coordinates(face_mesh)\n",
        "      vertex_normals     = get_vertex_normals(face_mesh)\n",
        "      dihedral_angles    = generate_dihedral_angles(face_mesh)\n",
        "\n",
        "      print(\"Intrinsic Features ...\")\n",
        "      mesh_gaussian_curvature     = generate_gaussian_curvature(face_mesh)\n",
        "      eigen_vectors, eigen_values = generate_cot_eigen_vectors(face_mesh, device, 25)\n",
        "      hks_features                = HKS(eigen_vectors, eigen_values, 25)\n",
        "\n",
        "      vertex_coordinates = np.array(vertex_coordinates, copy=True)\n",
        "      vertex_normals     = np.array(vertex_normals, copy=True)\n",
        "      dihedral_angles    = np.array(dihedral_angles, copy=True)\n",
        "      eigen_vectors      = np.array(eigen_vectors, copy=True)\n",
        "\n",
        "      # The input features\n",
        "      features = torch.cat([\n",
        "          torch.from_numpy(vertex_coordinates).float(),\n",
        "          torch.from_numpy(vertex_normals).float(),\n",
        "          torch.from_numpy(dihedral_angles).float(),\n",
        "          mesh_gaussian_curvature.float(),\n",
        "          torch.from_numpy(eigen_vectors[:, 1:21]).float(),\n",
        "          torch.from_numpy(hks_features).float()\n",
        "      ], dim=1)\n",
        "\n",
        "      print(\"features : \",features.shape)\n",
        "\n",
        "      eigen_vectors     = eigen_vectors.astype(np.float32)  # Convert to float32 for compatibility\n",
        "      features_G_tensor = torch.from_numpy(eigen_vectors)[:, :k].T.to(device) @ features.to(device)\n",
        "      features_G        = torch.flatten(features_G_tensor, start_dim=0).tolist()\n",
        "\n",
        "      print(\"Input features : \",len(features_G))\n",
        "    except:\n",
        "      error_type = 3\n",
        "      error_message(error_type)\n",
        "      #sys.exit(1)\n",
        "\n",
        "    if error_type == 0:\n",
        "      try:\n",
        "        # Create inputs\n",
        "        my_dataset = CustomInferenceDataset(features_G)\n",
        "        # Create DataLoader\n",
        "        inputloader = DataLoader(my_dataset, batch_size=1)\n",
        "\n",
        "        # Use the loaded model for inference\n",
        "        print(\"Inference ...\")\n",
        "        for batch in inputloader:\n",
        "          with torch.no_grad():\n",
        "            output    = model_loaded(batch)\n",
        "            predicted = output.squeeze().item()\n",
        "            if predicted >= 0.5 :\n",
        "              print(\"Model output : is Fake. (\", predicted,\") \\n\")\n",
        "            else:\n",
        "              print(\"Model output : is Real. (\", predicted,\") \\n\")\n",
        "      except:\n",
        "        error_type = 4\n",
        "        error_message(error_type)\n",
        "\n",
        "  return predicted\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "print(\"Done ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yi4F5UYjYH4",
        "outputId": "59fca21b-58bc-44f6-9990-26fbdd6cab20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prediction:**"
      ],
      "metadata": {
        "id": "v06m_rIS7gr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/3DDGD/examples/man_bust.obj\"\n",
        "crop      = 0\n",
        "cropped_face, landmarks = crop_3D_face(file_path, crop)\n",
        "\n",
        "fig_mesh = plot_3d_mesh_landmarks(file_path, landmarks)\n",
        "fig_mesh.show()\n"
      ],
      "metadata": {
        "id": "PMC13osL2wPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot cropped_face\n",
        "fig_mesh_face = plot_3d_mesh(cropped_face)\n",
        "fig_mesh_face.show()\n"
      ],
      "metadata": {
        "id": "k-zd4Q1s5BMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "labels       = [\"Fake\", \"Real\"]\n",
        "model_name   = \"Mesh_MLP_MHA\"\n",
        "feature_type = \"G0\"\n",
        "predicted    = model_predict(cropped_face, model_name, feature_type)\n",
        "print(\"predicted : \",predicted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oEO3V-A3EI4",
        "outputId": "81e088c5-5a82-464b-a591-15e1fe4e2385"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the model ... ( Mesh_MLP_MHA )\n",
            "Extrinsic Features ...\n",
            "Intrinsic Features ...\n",
            "features :  torch.Size([7780, 39])\n",
            "Input features :  624\n",
            "Inference ...\n",
            "Model output : is Real. ( 0.024476613849401474 ) \n",
            "\n",
            "predicted :  0.024476613849401474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradio:**"
      ],
      "metadata": {
        "id": "vEEp238C7m-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "uaNbayXffEek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "print(gr.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6Y2I29FfOjO",
        "outputId": "9fef9aff-8f2e-47e8-837d-912ddfe4ce86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.33.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Run the code in the functions section again if required.\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def predict_3D_deepfake(mesh_file, model_name, feature_type, is_cropped):\n",
        "    crop = 1 if is_cropped == \"1\" else 0\n",
        "\n",
        "    # Ensure the uploaded file path is passed correctly\n",
        "    file_path = mesh_file.name  # In Google Colab, this is how you get the file path\n",
        "\n",
        "    cropped_face, landmarks = crop_3D_face(file_path, crop)\n",
        "\n",
        "    fig_mesh      = plot_3d_mesh_landmarks(file_path, landmarks)\n",
        "    fig_mesh_face = plot_3d_mesh(cropped_face)\n",
        "\n",
        "    # Prediction\n",
        "    labels     = [\"Fake\", \"Real\"]\n",
        "    predicted  = model_predict(cropped_face, model_name, feature_type)\n",
        "    prediction = [predicted, 1 - predicted]\n",
        "    dictionary = dict(zip(labels, map(float, prediction)))\n",
        "\n",
        "    return fig_mesh, fig_mesh_face, dictionary\n",
        "\n",
        "\n",
        "# Creating the Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict_3D_deepfake,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload .obj file\", file_types=[\".obj\"]),\n",
        "        gr.Dropdown([\"Tabtransformer\", \"Mesh_MLP_MHA\"], label=\"Model\"),\n",
        "        gr.Dropdown([\"G0\", \"G1\", \"G2\"], label=\"Feature Type\"),\n",
        "        gr.Dropdown([\"0\", \"1\"], label=\"Is Cropped?\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Plot(label=\"3D Mesh Viewer\"),\n",
        "        gr.Plot(label=\"3D Cropped Face\"),\n",
        "        gr.Label(label=\"Result : \", num_top_classes=2),\n",
        "    ],\n",
        "    theme=gr.themes.Default(primary_hue=\"red\", secondary_hue=\"pink\"),\n",
        "    title=\"3D Deepfake Detection\",\n",
        "    description=\"Upload a .obj :\",\n",
        "    article=\"Â© 2025 Hichem Felouat - hichemfel@nii.ac.jp . All rights reserved.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio in Google Colab\n",
        "demo.launch(share=True, debug=True)  #(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "a_Wd427xAFtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}